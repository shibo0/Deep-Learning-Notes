ImageNet Classification with Deep Convolutional Neural Networks

> Krizhevsky, Alex et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM 60 (2012): 84 - 90.

[PDF](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

**摘要**

_我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。
在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率，这比以前的先进水平要好得多。
具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。
为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，
我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这个模型的一个变种，
取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。_

## 1 Introduction

为了从数以百万计的图像中了解成千上万的物体，我们需要一个具有较大学习能力的模型。
然而，物体识别任务的巨大复杂性意味着，即使像ImageNet这样大的数据集也不能完美解决这个问题，
所以我们的模型也应该有大量的先验知识来弥补我们数据集不足的问题。
卷积神经网络（CNN）就构成了一类这样的模型[16,11,13,18,15,22,26]。它们的容量可以通过改变它们的深度和宽度来控制，
并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。
因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。

contributions:

1. 我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。

2. 我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。

3. 我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3节。

4. 我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。

5. 我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：
我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。

## 2 The Dataset

ImageNet是一个由超过1500万张标记的高分辨率图像组成的数据集，属于大约22000个类别。
ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。

## 3 The Architecture

AlexNet模型结构如下，包含8个可学习层-五个卷积层和三个全连接层。

![architecture of AlexNet](https://img-blog.csdnimg.cn/4c0f241152f447a3af419a8adfbc7358.png)

### 3.1 ReLU Nonlinearity

对一个神经元模型的输出的常规套路是，给他接上一个激活函数：
$f(x)=tanh(x)$或者
$f(x)=(1+e^{−x})^{−1}$。就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如
$f(x)=max(0,x)$慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。
使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。

### 3.2 Training on Multiple GPUs 多GPU训练

单个GTX580 GPU只有3GB内存，这限制了在其上训练的网络的最大规模。可以在上面进行训练。事实证明，120万个训练实例足以训练网络
而这些网络太大，无法在一个GPU上安装。因此，我们将网络分散在两个GPU上。

### 3.3 Local Response Normalization 局部响应归一化

![在这里插入图片描述](https://img-blog.csdnimg.cn/6d45dc78a1704e24aa82fedad3659b0b.png)

### 3.4 Overlapping Pooling 重叠池化

CNN中的池化层汇集了相同内核映射中相邻神经元组的输出。我们设置
$z\*z$大小的池化窗，步长是s，如果s<z，那么此时我们就得到了重叠池化。
在训练过程中，我们普遍观察到，具有重叠池化的模型过拟合的难度略大。

### 3.5 整体架构

如上图

## 4 Reducing Overfitting

### 4.1 Data Augmentation

应用两种数据增强方式：一种是图片平移和镜像映射，一种是改变RGB通道的强度

### 4.2 Dropout

There is, however, a very efficient version of model combination that only costs about a
factor of two during training.
然而，有一种非常有效的模型组合方法，在训练期间，只需要消耗1/2的参数。这个新发现的技术叫做“Dropout”[10]，

## 5 训练细节

batch size 128, 动量0.9， 权重衰减0.0005

初始化：使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。
这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。

学习率：对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，
将学习速率除以10.学习率初始时设为0.01，并且在终止前减少3次。

我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA GTX 580 3GB GPU上花费了大约5到6天的时间。
