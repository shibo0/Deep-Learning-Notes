# Gradient-Based Learning Applied to Document Recognition

> Reference： LeCun, Y.; Bottou, L.; Bengio, Y. & Haffner, P. (1998). Gradient-based learning applied to document recognition.Proceedings of the IEEE. 86(11): 2278 - 2324.

摘要：
基于反向传播算法机制的多层神经网络已经取得最好的案例。基于反向传播机制可以用于综合复杂决策去分类高维模式，例如手写数字识别。这篇论文总结了在手写数字识别任务上不同的方法。
卷积神经网络是专门为处理二维图形的可变性而设计的，实验表明其性能优于所有其他技术。同时提出一个新的学习范式，即Graph Transformer Networks，允许多模块系统基于梯度方法全集训练，以达到最优的性能指标。

## 1 Introduction

### A Learning from Data

当前最流行的自动机器学习方法是梯度学习。学习机器计算函数
$Y^P=F(Z^P, W)$，
其中
$Z^p$
是第p个输入模式，W表示系统可调参数。
在一个模式识别中，输出
$Y^p$可以被解释为模式
$Z^p$
的类别标签，或解释为与每个类别相关的分数或概率。
一个损失函数
$E^p=D(D^p,F(W,Z^p)$，衡量模式正确或者期望的输出
$D^p$和系统预测的输出之间的差异，
$D^p$是模式
$Z^p$的"正确"或期望输出。
平均损失函数
$E_{train}(W)$是一组被称为训练集
$\{(Z^1,D^1),...(Z^P,D^P)\}$的标签实例的错误率
$E^p$的平均值。
学习问题包含发现
$W$值使其最小化
$E_{train}(W)$。

在实践中，系统在训练集上的表现没有什么意义。
更相关的衡量标准是当它在实践中会被使用时系统的错误率，这个性能是通过测量一组样本的准确度来估计。
与训练集不相干的样本集，称为测试集。许多理论和实验工作[3], [4], [5]已经表明，训练集Etest的预期错误率和训练集Etrain的错误率之间的差距随着训练样本数量的增加而减少，大约是为
$E_{test}-E_{train}=k(h/P)^{\alpha}$
$P$是训练集样本数，
$h$是衡量机器的 "有效能力 "或复杂性的指标[6], [7]。
$\alpha$是0.5到1.0之间的数字。
$k$是常量。
当训练样本数量增加时，这个差距总是减小。此外，随着容量
$h$的增加，
$E_{train}$减少。因此，增大容量
$h$时，存在
$E_{train}$的减小与间隙的增大之间的交易trade-off，
其最优值为容量
$h$,以达到最低泛化误差。大多数学习算法也试图最小化
$E_{train}$和对差距的一些估计。它的正式版本叫做结构风险最小化[6][7]。
在实践中，结构性风险最小化通过最小化
$E_{train} + \beta H(W)$来实现，其中函数H(W)称为正则化函数，
$beta$为一个常数。
$H(W)$的选择使得属于参数空间的高容量子集的参数
$W$取较大的值。最小化
$H(W)$实际上限制了参数可访问子集的容量空间，
从而控制最小化训练误差和最小化训练误差与测试误差之间的期望差距之间的权衡。

### B. Gradient-Based Learning

基于梯度的学习利用了这样一个事实，即最小化一个相当平滑的连续函数通常比最小化一个离散(组合)函数容易得多。
最简单的最小化程序是梯度下降算法，其中$W$被迭代调节，如下：

$W_k=W_{k-1}-\eta \frac{\partial E(W)}{\partial W}$

在最简单的情况下，
$eta$是一个标量常数。更复杂的程序使用变量
$eta$，或将其替换为对角矩阵，或将其替换为牛顿或准牛顿方法中逆Hessian矩阵的估计。共轭梯度法[8]也可以使用。
然而，附录B表明，尽管文献中有许多相反的说法，这些二阶方法对大型学习机的有用性是非常有限的。
一个流行的最小化程序是随机梯度算法SGD，也称为在线更新。它包括使用平均梯度的有噪声或近似版本更新参数向量。在最常见的例子中，W是基于单个样本进行更新的：

$W_k=W_{k-1}-\eta \frac{\partial E^{p_k}(W)}{\partial W}$

### C. Gradient Back-Propagation

自20世纪50年代末以来，基于梯度的学习过程一直在使用，但它们大多局限于线性系统。这种简单的梯度下降技术对于复杂的机器学习任务的惊人有用性直到以下三个方面才被广泛认识到。

1. 损失函数局部最小值的存在似乎不是实践中的主要问题。

2. Rumelhart等人的著作-反向传播算法。

3. 在带有sigmoid单元多层次神经网络中应用反向传播程序可以解决复杂的学习任务。

### D. Learning in Real Handwriting Recognition Systems

孤立手写字符识别已经在文献中得到了广泛的研究(参见[23]，[24]的评论)，并且是神经网络[25]的早期成功应用之一。在第三节中报告了识别单个手写数字的比较实验。
他们表明，使用基于梯度的学习训练的神经网络在相同的数据上比所有其他测试方法表现得更好。最好的神经网络，称为卷积网络，旨在学习直接从像素图像中提取相关特征。

### E. Global ly Trainable Systems

如前所述，大多数实用的模式识别系统由多个模块组成。更好的替代方法是以某种方式训练整个系统，以便最小化全局错误度量，
例如文档级别上字符分错的概率。理想情况下，我们希望找到一个全局损失函数相对于系统中所有参数的最小值。
如果衡量性能的损失函数
$E$相对于系统的可调参数
$W$是可微的，我们可以使用基于梯度的学习得到
$E$的局部最小值。
然而，这个系统的规模和复杂性似乎会让这个问题变得棘手。

## 2 Convolutional Neural Networks for Isolated Character recognition

用梯度下降训练的多层网络从大量的例子中学习复杂的、高维的、非线性的映射的能力使它们成为图像识别任务的明显候选者。
在传统的模式识别模型中，手工设计的特征提取器从输入中收集相关信息并消除不相关的变量。一个可训练的分类器然后将得到的特征向量分类。
在这个方案中，标准的、全连接的多层网络可以使用分类器。一个可能更有趣的方案是尽可能多地依赖特征提取器本身的学习。
在字符识别的情况下，网络可以被输入几乎原始的输入(例如，尺寸标准化的图像)。虽然这可以用普通的全连接前馈网络完成，并在字符识别等任务中取得一些成功，但存在问题。

首先，典型的图片通常具有数百个像素，用全连接网络处理这些数据需要包含数万个参数，如此大量的参数增加了系统容量，因此也需要大量的训练集。
此外，也需要大量的存储器容量去储存这些参数。
其次，全连接架构的一个缺陷是，输入的拓扑结构被完全忽略了。

### A. Convolutional Networks

卷积网络结合了三种架构思想来确保一定程度的移位、规模和失真不变性:局部接受域、共享权重(或权重复制)和空间或时间子采样。
输入平面接收大约尺寸规范化和居中的字符图像。一层中的每个单元接收来自前一层中位于小邻域的一组单元的输入。
将单元与输入上的局部接受区连接起来的想法可以追溯到60年代早期的感知机，几乎与Hubel和Wiesel在猫的视觉系统[30]中发现的局部敏感、定向选择神经元同时出现。

### B. LeNet-5

![LeNet-5](https://img-blog.csdnimg.cn/d6bac0bbbdab440ea771cdcd79dc7427.png)

原始架构除去输入层外一共7层，输入是32\*32的灰度图，也就是说输入大小是32\*32\*1，并对输入图片进行正则化，正则化后白色（背景）为-0.1，黑色为1.175，最终均值大约为0，方差大约为1.

第一层C1，采用6个5\*5的卷积核按步长为1，0填充进行卷积（没记错的话，那时候的卷积还没有对padding进行研究）。那么参数只有卷积核的权重和偏置，那么总的参数个数就是5\*5\*6+6=156个参数，卷积后的特征图大小为28\*28\*6，每个像素都与前一层的5\*5个像素和1一个bias有连接，有6∗(5∗5+1)∗(28∗28)个连接。

第二层S2，对6个特征图进行下采样，即对feature map做2\*2，步长为2，0填充的池化，池化层对池化窗口的四个值进行加和来进行下采样，
并将下采样后的特征图乘以一个可训练系数加上偏置，并通过sigmoid函数进行非线性映射。最终池化后的特征图大小为14\*14\*6，那么总的参数个数就是2\*6=12个参数，
65\*14\*14=5880个连接。 池化层本身通过下采样减少了数据量，保留了有用信息，同时也减少了模型参数，降低了过拟合。

第三层C3，采用16个5\*5的卷积核按步长为1，0填充进行卷积（实际上没有添加padding的卷积核本身就带有下采样效果）。
最终总的参数个数就是6∗(3∗5∗5+1)+6∗(4∗5∗5+1)+3∗(4∗5∗5+1)+1∗(6∗5∗5+1)=1516个参数，151600个连接。原文中为了控制连接数量和打破网络对称性，
并没有对
$S_2$和
$C_3$所有进行连接,而是按照下图所示规则：

![在这里插入图片描述](https://img-blog.csdnimg.cn/606e453693594219a008bdf315b22a48.png)

原文中对这个模式的解释为：前六个（即上图横坐标0-5）卷积核提取
$S_2$中 连续三个子集中的特征，后六个（即上图横坐标6-11）卷积核提取
$S_2$中 连续四个子集中的特征，后三个（即上图横坐标12-14）卷积核提取
$S_2$中离散的四个子集中的特征，最后一个提取全图特征。（现在貌似没看到这么做了，可能比较麻烦，效果也不是特别显著

第四层S4，对16个特征图进行下采样，即对feature map做2\*2，步长为2，0填充的池化。最终池化后的特征图大小为5\*5\*16，那么总的参数个数就是2\*16=32个参数，
16\*（554+25）=2000个连接。

第五层C5，采用120个5\*5的卷积核按步长为1，0填充进行卷积。那么总的参数个数就是（5\*5\*16+1）\*120=48120个参数，同样由48120个连接卷积后的特征图大小为1\*1\*120。

第六层FC6是一个全连接层，该层采用
$a_i=f(W \cdot x_i),f(a) = A \tanh (S a)$其中，A是振幅（原始论文设值为1.7159），S是相位。
全连接层共有12084+84=10164个参数。

第七层输出层FC7，采用RBF函数作为损失函数：
$y_i=\sum_i(x_i-w_{ij})^2$用于度量输入和参数的距离

原论文中的损失函数采用MSE，并添加了一个惩罚项（后文查看为什么要添加惩罚项），计算公式为：
$E(W)=\frac{1}{P} \sum_{p=1}^{P} y_{D^P}(Z^P,W)+log(e^{-j}+\sum_i e^{-y_i(z^p,W)}))$
右边这个对数部分就是惩罚项，且小于等于左边。但实际上现在很多对于LeNet-5的实现是没有这一部分的，因为效果不明显。
